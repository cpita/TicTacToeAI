'''
Author: Carlos Pita
2020
'''
import logging
import os
import multiprocessing as mp
import time

from core.models import TicTacToe
from core.policies import NeuralNetPolicy, RandomPolicy, OneStepPolicy
from utils import WinRatioMetrics, TimePerformanceMetrics, perform_analytics_on_sweep, perform_analytics

'''The params_sequence() generator will run all the combinations of these parameters'''
NUM_LAYERS = [5]
NEURONS_PER_LAYER = [16]
ALPHA = [.01]
ALPHA_DECAY = [None]
'''Set to None for no eps-greedy'''
EPS_DECAY = [None]
EPS_FOR_UPDATE = [100]
TOTAL_ITERS = [1000]
"""Only for file naming purposes, will go at the beginning of each file name"""
NAMESPACE = 'my ttt agent'


def train(env, p1, p2, name, eps_for_update, total_iters):
    """Trains one combination of parameters
    :param env: Environment the two agents interact on
    :type env: TicTacToe
    :param p1: Policy used by Player 1 (X)
    :type p1: Policy
    :param p2: Policy used by Player 2 (O)
    :type p2: Policy
    :param name: Name given to the files generated by this configuration
    :type name: str
    :param eps_for_update: Number of episodes that need to be sampled for a gradient descent update to happen
    :type eps_for_update: int
    :param total_iters: Number of training steps that are performed in total
    :type total_iters: int
    """

    '''Just some metrics, can safely ignore'''
    win_ratio_metrics = WinRatioMetrics()
    time_performance_metrics = TimePerformanceMetrics()
    start_time = time.time()
    if not os.path.exists(f'data/{name}'):
        os.makedirs(f'data/{name}')
    logging.basicConfig(level=logging.INFO, filename=f'data/{name}/{name}.log', format='%(asctime)s, %(message)s')
    print(f'Started training of {name}')

    '''This loop will run over each training step / gradient descent step'''
    for i in range(total_iters):

        time_performance_metrics.start()
        win_ratio_metrics.reset()

        '''This loop will run over each episode, and finish once a training step is going to be made'''
        for _ in range(eps_for_update):

            '''
            Flow of one episode. It seems too complex because it enables the use of the same environment for two
            reinforcement learning agents at the same time (self play). It is not that trivial when to assign rewards.
            '''
            first_iter = True
            while True:
                p1_state = env.current_state
                p1_action = p1.sample(p1_state)
                env.play(p1_action)
                '''Check if the move player1 did leads to a terminal state. There is a terminal state if the reward is
                 not None'''
                p1_reward = env.get_reward_player1()
                if p1_reward is not None:
                    win_ratio_metrics.register_win(p1_reward)
                    p1.collect(p1_state, p1_action, p1_reward)
                    p2_reward = p1_reward * (-1)
                    p2.collect(p2_state, p2_action, p2_reward)
                    break

                '''Here we must make a distinction for the first iteration because p2_state and p2_action have not been
                 defined yet'''
                if not first_iter:
                    p2.collect(p2_state, p2_action, 0)
                else:
                    first_iter = False

                p2_state = env.current_state
                p2_action = p2.sample(p2_state)
                env.play(p2_action)
                p1_reward = env.get_reward_player1()
                p1.collect(p1_state, p1_action, p1_reward or 0)
                '''Check if the move player2 did leads to a terminal state'''
                if p1_reward is not None:
                    win_ratio_metrics.register_win(p1_reward)
                    p2_reward = p1_reward * (-1)
                    p2.collect(p2_state, p2_action, p2_reward)
                    break

            '''Update the Monte Carlo estimates of the afterstates encountered in the last episode'''
            p1.update()
            p2.update()
            env.reset()

        '''With these calls we update the gradient for the agents that are using a NeuralNetPolicy. 
        Here is where the policy actually changes'''
        if p1.model:
            p1.update_gradient()
            if p1.trainable:
                p1.model.save_weights(name, p1.turn)
        if p2.model:
            p2.update_gradient()
            if p2.trainable:
                p2.model.save_weights(name, p2.turn)

        '''Just some metrics, can safely ignore'''
        seconds = time_performance_metrics.get_timer()
        p1_wins, p2_wins, ties = win_ratio_metrics.get_results()
        logging.info(
            f'Batch of {eps_for_update} episodes took {seconds} seconds for {p1_wins} player1 wins, {p2_wins} player2 wins and {ties} ties')
        win_ratio_metrics.update_win_ratio(100*(p1_wins/eps_for_update), 100*(p2_wins/eps_for_update), (i + 1) * eps_for_update)
        win_ratio_metrics.win_ratio.to_csv(f'data/{name}/win_ratio_{name}.csv', index=False)
        print('\033[H\033[J')
        print(f'Progress for training configuration {name}: {round((i / total_iters) * 100, 1)}%')
        print(f'Win ratio of player 1: {100*(p1_wins/eps_for_update)}%')
        print(f'Win ratio of player 2: {100 * (p2_wins / eps_for_update)}%')
        print(f'Ties: {100 * (ties / eps_for_update)}%')

    '''Just some metrics, can safely ignore'''
    training_time = time.time() - start_time
    logging.info(f'Total training time: {training_time} seconds')
    print(f'Finished training of {name} in {training_time} seconds')


def params_sequence():
    """Yields every combination of the params"""
    for num_layers in NUM_LAYERS:
        for neurons_per_layer in NEURONS_PER_LAYER:
            for alpha in ALPHA:
                for alpha_decay_factor in ALPHA_DECAY:
                    for epsilon_decay_factor in EPS_DECAY:
                        for eps_for_update in EPS_FOR_UPDATE:
                            for total_iters in TOTAL_ITERS:
                                environment = TicTacToe()
                                '''
                                This way of naming the files is recommended when trying performing parameter sweeps
                                '''
                                # name = f'{NAMESPACE}_l{num_layers}_n{neurons_per_layer}_a{alpha}_d{alpha_decay_factor}_ed{epsilon_decay_factor}_e{eps_for_update}_i{total_iters}'
                                name = NAMESPACE
                                p1 = OneStepPolicy(environment)
                                p2 = make_NeuralNetPolicy(environment, neurons_per_layer, num_layers, alpha=alpha,
                                                          alpha_decay_factor=alpha_decay_factor,
                                                          eps_decay_factor=epsilon_decay_factor)

                                p1.turn = 1
                                p2.turn = -1
                                yield environment, p1, p2, name, eps_for_update, total_iters


def make_NeuralNetPolicy(environment, neurons_per_layer, num_layers, alpha=.01, gamma=.95, alpha_decay_factor=None,
                        eps_decay_factor=None, trainable=True):

    return NeuralNetPolicy(environment, [neurons_per_layer for _ in range(num_layers)], alpha=alpha, gamma=gamma,
                           alpha_decay_factor=alpha_decay_factor, eps_decay_factor=eps_decay_factor, trainable=trainable)


if __name__ == '__main__':

    params_gen = params_sequence()
    start_time_total = time.time()

    '''Uncomment for multiprocessing mode'''
    '''processes = [mp.Process(target=train, args=params) for params in params_gen]

    for process in processes:
        process.start()

    for process in processes:
        process.join()'''


    '''Single Process mode'''
    for params in params_gen:
        train(*params)

    print(f'Finished Training in {time.time() - start_time_total} seconds')

    '''Uncomment if you want to see graphs of Player 1's win rate for one parameter setting'''
    #perform_analytics(NAMESPACE)

    '''Uncomment if you want to see graphs of Player 1's win rate for a parameter sweep'''
    #perform_analytics_on_sweep(NAMESPACE, NUM_LAYERS, NEURONS_PER_LAYER, ALPHA, ALPHA_DECAY, EPS_DECAY, EPS_FOR_UPDATE,
    #                           TOTAL_ITERS)















